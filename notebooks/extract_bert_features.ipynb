{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/beckmann/fairseq\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from IPython.display import Image, display\n",
    "import mlflow\n",
    "from fairseq.data import dictionary\n",
    "from sklearn.metrics import pairwise_distances_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.data import (\n",
    "    data_utils,\n",
    "    Dictionary,\n",
    "    PadDataset,\n",
    "    PrependTokenDataset,\n",
    "    TokenBlockDataset,\n",
    "    EmbeddingDataset\n",
    ")\n",
    "from fairseq.data import iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "roberta = RobertaModel.from_pretrained('checkpoints/', 'checkpoint_best.pt', '/mnt/tamedia/video_concierge/new_imnet_10k')\n",
    "roberta.cuda()\n",
    "assert isinstance(roberta.model, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(split_path):\n",
    "    # TOKEN DATASET\n",
    "    # dictionary\n",
    "    dictionary = Dictionary.load(os.path.join(split_path.rsplit('/',1)[0], 'dict.txt'))\n",
    "    token_dataset = data_utils.load_indexed_dataset(\n",
    "            split_path,\n",
    "            dictionary,\n",
    "            None,\n",
    "            combine=False,\n",
    "    )\n",
    "    # create continuous blocks of tokens\n",
    "    token_dataset = TokenBlockDataset(\n",
    "        token_dataset,\n",
    "        token_dataset.sizes,\n",
    "        512 - 1,  # one less for <s>\n",
    "        pad=dictionary.pad(),\n",
    "        eos=dictionary.eos(),\n",
    "        break_mode='eos',\n",
    "    )\n",
    "    # prepend beginning-of-sentence token (<s>, equiv. to [CLS] in BERT)\n",
    "    token_dataset = PrependTokenDataset(token_dataset, dictionary.bos())\n",
    "    token_dataset = PadDataset(token_dataset, pad_idx=dictionary.pad(), left_pad=False)\n",
    "        \n",
    "    # EMBEDDING DATASET\n",
    "    embs = torch.load(split_path + '.features')\n",
    "    embedding_dataset = EmbeddingDataset(embs, pad_idx=0, left_pad=False)\n",
    "    \n",
    "    # COUNT DATASET\n",
    "    # load counts\n",
    "    thresh = 100\n",
    "    with open(split_path + '.counts') as count_file:\n",
    "        lines = [line.rstrip() for line in count_file]\n",
    "        counts = [line.split(' ') for line in lines]\n",
    "        for i, count in enumerate(counts):\n",
    "            count = [int(el) for el in count]\n",
    "            counts[i] = [el if el < thresh else thresh for el in count]\n",
    "            counts[i] = torch.LongTensor(np.concatenate([[0],counts[i],[0]]))\n",
    "    count_dataset = PadDataset(counts, pad_idx=0, left_pad=False)\n",
    "    \n",
    "    return token_dataset, embedding_dataset, count_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "token_dataset, embedding_dataset, count_dataset = load_datasets('/mnt/tamedia/video_concierge/new_imnet_10k/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = len(token_dataset)\n",
    "\n",
    "# batch sampler\n",
    "batch_sampler = []\n",
    "batch_size = 1\n",
    "for i in range(0, epoch_size-batch_size, batch_size):\n",
    "    batch_sampler.append(list(range(i, i+batch_size)))\n",
    "batch_sampler.append(list(range(i, epoch_size)))\n",
    "\n",
    "# iterators\n",
    "token_iterator = iterators.EpochBatchIterator(\n",
    "        dataset=token_dataset,\n",
    "        collate_fn=token_dataset.collater,\n",
    "        batch_sampler=batch_sampler\n",
    ").next_epoch_itr(shuffle=False)\n",
    "\n",
    "embedding_iterator = iterators.EpochBatchIterator(\n",
    "        dataset=embedding_dataset,\n",
    "        collate_fn=embedding_dataset.collater,\n",
    "        batch_sampler=batch_sampler\n",
    ").next_epoch_itr(shuffle=False)\n",
    "\n",
    "count_iterator = iterators.EpochBatchIterator(\n",
    "        dataset=count_dataset,\n",
    "        collate_fn=count_dataset.collater,\n",
    "        batch_sampler=batch_sampler\n",
    ").next_epoch_itr(shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_features = []\n",
    "for token, embedding, count in zip(token_iterator, embedding_iterator, count_iterator):\n",
    "    bert_features.append(torch.mean(roberta.extract_features(token, count, embedding), axis=0)[0].cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize close videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keyframes_for_vid(video_path):\n",
    "    frames = sorted(glob(video_path + '/*'))\n",
    "    for frame in frames:\n",
    "        display(Image(filename=frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances_argmin_gpu(x, y, bsz=10000, cuda_device='cuda:0'):\n",
    "    argmins = np.zeros(len(x))\n",
    "    device = torch.device(cuda_device)\n",
    "    yy = torch.from_numpy(y).float().to(device)\n",
    "    for i in range(0, len(x), bsz):\n",
    "        xx = torch.from_numpy(x[i:i+bsz,:]).float().to(device)\n",
    "        out = torch.cdist(xx,yy)\n",
    "        a = torch.argmin(out, 1).cpu().numpy()\n",
    "        argmins[i:i+len(a)] = a\n",
    "        del xx\n",
    "    return argmins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/mnt/tamedia/video_concierge/bert_data/kf_data/valid'\n",
    "videos = sorted(glob('{}/*'.format(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidnum=26\n",
    "show_keyframes_for_vid(videos[vidnum])\n",
    "argmins = pairwise_distances_argmin_gpu(np.expand_dims(bert_features[vidnum], axis=0), np.delete(bert_features, vidnum, axis=0), bsz=1)\n",
    "print('_____________________________________________________________________________________________________________')\n",
    "print(int(argmins[0]))\n",
    "show_keyframes_for_vid(np.delete(videos, vidnum)[int(argmins[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with averaged features\n",
    "averaged_features = []\n",
    "for embs in embedding_iterator:\n",
    "    averaged_features.append(torch.mean(embs, axis=1)[0].cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidnum=26\n",
    "show_keyframes_for_vid(videos[vidnum])\n",
    "argmins = pairwise_distances_argmin_gpu(np.expand_dims(averaged_features[vidnum], axis=0), np.delete(averaged_features, vidnum, axis=0), bsz=1)\n",
    "print('_____________________________________________________________________________________________________________')\n",
    "print(int(argmins[0]))\n",
    "show_keyframes_for_vid(np.delete(videos, vidnum)[int(argmins[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
